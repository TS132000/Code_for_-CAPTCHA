{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,re,sys,math\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold  \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = 'data/dsjtzs_txfz_training.txt'\n",
    "test_path = 'data/dsjtzs_txfz_test1.txt'\n",
    "test_path_b = 'data/dsjtzs_txfz_testB.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def inf_to_nan(x):\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == float(\"inf\") or x[i] == float(\"-inf\"):\n",
    "            x[i] = float('nan') # or x or return whatever makes sense\n",
    "    return x\n",
    "\n",
    "\n",
    "def sample_data(a,size = 0.7):\n",
    "    np.random.seed(1)\n",
    "    len_data = len(a)\n",
    "    #size = np.random.random_sample()  #抽样比例随机\n",
    "    size = int(size *len_data)\n",
    "    if size < 1:\n",
    "        size = 1\n",
    "    index = np.sort((np.random.random(size)*len_data).astype(int))\n",
    "    return np.array(a)[index]\n",
    "def feature(path,extend=True):\n",
    "    train = pd.read_csv(path,sep=' ',header=None,encoding='utf-8',names=['id','data','target','label'])\n",
    "    train['data'] = train['data'].apply(lambda x:[list(map(float,point.split(','))) for point in x.split(';')[:-1]])\n",
    "\n",
    "    \n",
    "    if extend:\n",
    "        #######扩充数据集#####\n",
    "\n",
    "\n",
    "        train_new_1 = train.copy()\n",
    "        train_new_1['data'] = train['data'].apply(lambda x :sample_data(x,size = 0.3))\n",
    "        train_new_1['id'] = train['id'] + 3000 \n",
    "\n",
    "        train=train.append(train_new_1)\n",
    "\n",
    "\n",
    "        train_new_1 = train.copy()\n",
    "        train_new_1['data'] = train['data'].apply(lambda x :sample_data(x,size = 0.7))\n",
    "        train_new_1['id'] = train['id'] + 3000 \n",
    "        train=train.append(train_new_1)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    train['target'] = train['target'].apply(lambda x: list(map(float,x.split(\",\"))))\n",
    "    train['data_x'] = train['data'].apply(lambda x: [i[0] for i in x ])\n",
    "    train['data_y'] = train['data'].apply(lambda x: [i[1] for i in x ])\n",
    "    train['data_t'] = train['data'].apply(lambda x: [i[2] for i in x ])\n",
    "    train['data_x'] = train['data_x'].apply(lambda x: np.array(x))\n",
    "    train['data_y'] = train['data_y'].apply(lambda x: np.array(x))\n",
    "    train['data_t'] = train['data_t'].apply(lambda x: np.array(x))\n",
    "    \n",
    "    del train['data']\n",
    "\n",
    "    \n",
    "    train['target_x'] = train['target'].apply(lambda x: x[0])\n",
    "    train['target_y'] = train['target'].apply(lambda x: x[1])\n",
    "    #delt \n",
    "    train['delt_x'] = train['data_x'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_y'] = train['data_y'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_t'] = train['data_t'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_xy'] = train.delt_x**2 + train.delt_y**2\n",
    "    train['delt_xy'] = train['delt_xy'].apply(lambda x: np.sqrt(x))\n",
    "    #speed\n",
    "    train['speed_x'] = (train.delt_x/train.delt_t).apply(lambda x :inf_to_nan(x)).apply(lambda x :np.nan_to_num(np.array(x)))\n",
    "    train['speed_y'] = (train.delt_y/train.delt_t).apply(lambda x :inf_to_nan(x)).apply(lambda x :np.nan_to_num(np.array(x)))\n",
    "    train['speed_xy'] = (train.delt_xy/train.delt_t).apply(lambda x :inf_to_nan(x)).apply(lambda x :np.nan_to_num(np.array(x)))\n",
    "\n",
    "    #delt_speed\n",
    "    train['delt_speed_x'] = train['speed_x'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_speed_y'] = train['speed_y'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_speed_xy'] = train['speed_xy'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_speed_t'] = train['delt_t'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "\n",
    "    #acc\n",
    "    train['acc_speed_x'] = (train.delt_speed_x/train.delt_speed_t).apply(lambda x :inf_to_nan(x)).apply(lambda x :np.nan_to_num(np.array(x)))\n",
    "    train['acc_speed_y'] = (train.delt_speed_y/train.delt_speed_t).apply(lambda x :inf_to_nan(x)).apply(lambda x :np.nan_to_num(np.array(x)))\n",
    "    train['acc_speed_xy'] = (train.delt_speed_xy/train.delt_speed_t).apply(lambda x :inf_to_nan(x)).apply(lambda x :np.nan_to_num(np.array(x)))\n",
    "\n",
    "    #轨迹长度\n",
    "    train['len_x'] =  train['data_x'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "    #first\n",
    "    train['first_data_x'] = train['data_x'].apply(lambda x : x[0])\n",
    "    train['first_speed_x'] = train['speed_x'].apply(lambda x: x[0] if x.shape[0] > 0 else 0)\n",
    "    train['first_data_y'] = train['data_y'].apply(lambda x : x[0])\n",
    "    train['first_delt_t'] = train['delt_t'].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "     \n",
    "    train['time_delta_min'] = train['delt_t'].map(lambda x: x.min() if x.shape[0] > 0 else 0)\n",
    "    train['distance_deltas_max'] = train['delt_xy'].map(lambda x: x.max() if x.shape[0] > 0 else 0)\n",
    "    train['median_speed'] = train['speed_x'].map(lambda x: np.median(x) if x.shape[0] > 0 else 0)\n",
    "    train['xs_delta_var'] = train['delt_x'].map(lambda x: x.var() if x.shape[0] > 0 else 0)\n",
    "    train['xs_delta_max'] = train['delt_x'].map(lambda x: x.max() if x.shape[0] > 0 else 0)\n",
    "    train['x_min'] = train['data_x'].map(lambda x: x.min())\n",
    "    train['y_min'] = train['data_y'].map(lambda x: x.min())\n",
    "    \n",
    "    \n",
    "    #####################\n",
    "       # X_max与X_target关系\n",
    "    train['X_max'] = train['data_x'].map(lambda x: x.max() if x.shape[0] > 0 else 0)\n",
    "    train['get_target'] = train['X_max'] - train['target_x']\n",
    "    \n",
    "    #特征75 76\n",
    "    train['data_x_return'] = train['delt_x'].apply(lambda x: np.where(x<0)[0].shape[0]>0 if x.shape[0]>0 else False)\n",
    "    # #y坐标唯一值个数的标准差比平均值\n",
    "    train['data_y_unique_value'] = train['data_y'].map(\n",
    "            lambda x: np.unique(x,return_counts=True) if x.shape[0] > 0 else 0)\n",
    "    train['data_y_unique_value_stdmean'] = train['data_y_unique_value'].map(\n",
    "        lambda x: x[1].std()/x[1].mean() if x != 0 else 0)\n",
    "    del train['data_y_unique_value']\n",
    "    \n",
    "    \n",
    "    train['dxspeed_mean'] = train[\"delt_speed_x\"].apply(lambda x :np.mean(x) if x.shape[0] > 0 else 0)  #x方向速度差分的平均值\n",
    "    train['dxspeed_std']  = train[\"delt_speed_x\"].apply(lambda x :np.std(x) if x.shape[0] > 0 else 0)    #x方向速度差分的标准差\n",
    "    train['dxspeed_median']  = train[\"delt_speed_x\"].apply(lambda x :np.median(x) if x.shape[0] > 0 else 0)    \n",
    "    #x方向速度全局标准差与最后N个标准差比值\n",
    "    train['speed_xstd']  = train[\"speed_x\"].apply(lambda x :np.std(x) if x.shape[0] > 0 else 0)    #x方向速度差分的标准差\n",
    "    train['speed_x_laststd']  = train[\"speed_x\"].apply(lambda x :np.std(x[-9:]) if x.shape[0] > 0 else 0)    #x方向速度差分的标准差\n",
    "    train['speed_xstd_laststd'] = train['speed_xstd'] / train['speed_x_laststd']\n",
    "    train['speed_xstd_laststd'] = train['speed_xstd_laststd'].apply(lambda x : float('nan') if(x==float('-inf') or x==float('inf')) else x)\n",
    "    \n",
    "    ###0717\n",
    "    \n",
    "    #delt_speed\n",
    "    train['delt_acc_x'] = train['delt_speed_x'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    #train['delt_acc_y'] = train['delt_speed_y'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_acc_t'] = train['delt_speed_t'].apply(lambda x: np.array(x)[1:]-np.array(x)[:-1])\n",
    "    train['delt_acc_speed_x'] = (train.delt_acc_x/train.delt_acc_t).apply(lambda x :inf_to_nan(x)).apply(lambda x :np.nan_to_num(np.array(x)))\n",
    "    train['dxacc_mean'] = train[\"delt_acc_speed_x\"].apply(lambda x :np.mean(x) if x.shape[0] > 0 else 0)  #x方向速度差分的平均值\n",
    "    train['dxacc_std']  = train[\"delt_acc_speed_x\"].apply(lambda x :np.std(x) if x.shape[0] > 0 else 0)    #x方向速度差分的标准差\n",
    "    train['dxacc_median']  = train[\"delt_acc_speed_x\"].apply(lambda x :np.median(x) if x.shape[0] > 0 else 0)    \n",
    "    \n",
    "    \n",
    "    #0718 19\n",
    "    from scipy import stats\n",
    "    train['log_delt_x'] = train['data_x'].apply(lambda x: np.log1p(np.array(x)[1:]-np.array(x)[:-1]))\n",
    "    train['log_delt_y'] = train['data_y'].apply(lambda x: np.log1p(np.array(x)[1:]-np.array(x)[:-1]))\n",
    "    train['log_delt_t'] = train['data_t'].apply(lambda x: np.log1p(np.array(x)[1:]-np.array(x)[:-1]))\n",
    "    \n",
    "    train['xy_angles'] = train['log_delt_x'] - train['log_delt_y']\n",
    "    train['xt_angles'] = train['log_delt_x'] - train['log_delt_t']\n",
    "    train['xy_angles_kurtosis']= train['xy_angles'].apply(lambda x: stats.kurtosis(x))\n",
    "    train['xt_angles_max'] =train['xt_angles'].apply(lambda x :np.max(x) if x.shape[0] > 0 else 0)  #x方向速度差分的平均值\n",
    "    feats = ['id','label',\n",
    "           # 'len_x',\n",
    "    'first_data_x',\n",
    "     'first_speed_x',\n",
    "     'first_data_y',\n",
    "     'first_delt_t',\n",
    "     'time_delta_min',\n",
    "     'distance_deltas_max',\n",
    "     'median_speed',\n",
    "     'xs_delta_var',\n",
    "     'xs_delta_max',\n",
    "     'x_min',\n",
    "     'y_min',\n",
    "     'X_max',\n",
    "     'get_target',\n",
    "     'data_x_return',\n",
    "     'data_y_unique_value_stdmean',\n",
    "     'dxspeed_mean',\n",
    "     'dxspeed_std',\n",
    "     'dxspeed_median',\n",
    "           'speed_xstd',\n",
    "     'speed_x_laststd',\n",
    "     'speed_xstd_laststd',\n",
    "             \n",
    "     'dxacc_mean',    #715 feature\n",
    "     'dxacc_std', #gooooood\n",
    "     'xt_angles_max', #0718\n",
    "     'xy_angles_kurtosis',#0719\n",
    "\n",
    "             \n",
    "        ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customedscore(preds, dtrain):\n",
    "    label = dtrain.get_label()\n",
    "    pred = [int(i>=0.50) for i in preds]\n",
    "    confusion_matrixs = confusion_matrix(label, pred)\n",
    "    recall =float(confusion_matrixs[0][0]) / float(confusion_matrixs[0][1]+confusion_matrixs[0][0])\n",
    "    \n",
    "    precision = float(confusion_matrixs[0][0]) / float(confusion_matrixs[1][0]+confusion_matrixs[0][0])\n",
    "    F = 5*precision* recall/(2*precision+3*recall)*100\n",
    "    return 'FSCORE',float(F)\n",
    "\n",
    "def getscore(val_y, val_y_pre):\n",
    "    confusion_matrixs = confusion_matrix(val_y, val_y_pre)\n",
    "    recall =float(confusion_matrixs[0][0]) / float(confusion_matrixs[0][1]+confusion_matrixs[0][0])\n",
    "    precision = float(confusion_matrixs[0][0]) / float(confusion_matrixs[1][0]+confusion_matrixs[0][0])\n",
    "    # print(confusion_matrixs)\n",
    "    # print(\"recall:\",recall)\n",
    "    # print(\"precision:\",precision)\n",
    "    F = 5*precision* recall/(2*precision+3*recall)*100\n",
    "    # print(\"score :\",F)\n",
    "    return  recall , precision,F\n",
    "def xgb_pre(params,train_x,train_y,val_x):\n",
    "    import xgboost as xgb\n",
    "    #######################xgboost###########\n",
    "    trainset1 = xgb.DMatrix(train_x,label=train_y,missing=-100)\n",
    "    dval_x = xgb.DMatrix(val_x,missing=-100)\n",
    "    int_params = params[\"int_params\"]\n",
    "    num_boost_rounds = params[\"num_boost_rounds\"]\n",
    "    threshold = params[\"threshold\"]\n",
    "    model = xgb.train(int_params,trainset1,\n",
    "        num_boost_round=num_boost_rounds,feval = customedscore,maximize=False)\n",
    "    val_y_pre = model.predict(dval_x)\n",
    "    val_y_pred = [int(i >= threshold) for i in val_y_pre]\n",
    "\n",
    "    feature_score = model.get_fscore()\n",
    "    feature_score = sorted(feature_score.items(), key=lambda x:x[1],reverse=True)\n",
    "    print(feature_score)\n",
    "    return val_y_pre,val_y_pred\n",
    "\n",
    "\n",
    "def xgbcv(params,train_feature,train_label):\n",
    "    recall,precision,score =[],[],[] \n",
    "    l1 =[]\n",
    "    for i in range(10):\n",
    "        train_x,val_x ,train_y,val_y=train_test_split(train_feature,train_label, test_size=0.8,random_state=i)\n",
    "        val_y_pre, val_y_pred = xgb_pre(params,train_x,train_y,val_x)\n",
    "        r,p,s = getscore(val_y, val_y_pred)\n",
    "        recall.append(r);precision.append(p),score.append(s)\n",
    "    print(\"recall mean    : %.5f  recall var     : %.4f\"%(np.mean(recall),np.var(recall)))\n",
    "    print(\"precision mean : %.5f  precision var  : %.5f\"%(np.mean(precision),np.var(precision)))\n",
    "    print(\"score mean     :%.5f  score var      :%.5f\"%(np.mean(score),np.var(score)))\n",
    "def xgbcv2(train_feature1,train_feature2,train_label):\n",
    "    recall,precision,score =[],[],[] \n",
    "    l1 =[]\n",
    "    for i in range(10):\n",
    "        train_x1,val_x1 ,train_y,val_y=train_test_split(train_feature1,train_label, test_size=0.8,random_state=i)\n",
    "        train_x2,val_x2 ,train_y,val_y=train_test_split(train_feature2,train_label, test_size=0.8,random_state=i)\n",
    "        import xgboost as xgb\n",
    "        #######################xgboost###########\n",
    "        val_y_pre,val_y_pred = bagging(train_x1,val_x1 ,train_x2,val_x2 ,train_y)\n",
    "        r,p,s = getscore(val_y, val_y_pred)\n",
    "        recall.append(r);precision.append(p),score.append(s)\n",
    "    print(\"recall mean    : %.5f  recall var     : %.4f\"%(np.mean(recall),np.var(recall)))\n",
    "    print(\"precision mean : %.5f  precision var  : %.5f\"%(np.mean(precision),np.var(precision)))\n",
    "    print(\"score mean     :%.5f  score var      :%.5f\"%(np.mean(score),np.var(score)))\n",
    "\n",
    "\n",
    "def bagging(train_feature1,test_feature1,train_feature2,test_feature2,train_label):\n",
    "    threshold = 0.70\n",
    "    t = 0.45\n",
    "    val_y_pre1,val_y_pred1 = xgb_pre(params,train_feature1,train_label,test_feature1)\n",
    "    val_y_pre2,val_y_pred2 = xgb_pre(params,train_feature2,train_label,test_feature2)\n",
    "    val_y_pre =val_y_pre1*t+val_y_pre2*(1-t)\n",
    "    val_y_pred = [int(i >= threshold) for i in val_y_pre]\n",
    "    return val_y_pre, val_y_pred\n",
    "\n",
    "def bagging2(train_feature1,test_feature1,train_feature2,test_feature2,train_label,train_label2):\n",
    "    threshold = 0.65\n",
    "    t = 0.7\n",
    "    val_y_pre1,val_y_pred1 = xgb_pre(params,train_feature1,train_label,test_feature1)\n",
    "    val_y_pre2,val_y_pred2 = xgb_pre(params,train_feature2,train_label2,test_feature2)\n",
    "    val_y_pre =val_y_pre1*t+val_y_pre2*(1-t)\n",
    "    val_y_pred = [int(i >= threshold) for i in val_y_pre]\n",
    "    return val_y_pre, val_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train = feature(train_path,extend = False)\n",
    "train = feature(train_path,extend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test = feature(test_path,extend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_b = feature(test_path_b,extend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats2 = ['id','label',\n",
    "  \n",
    "'first_data_x', 'first_speed_x', 'first_data_y', 'first_delt_t', 'speed_xstd_laststd', 'X_max', 'get_target', #20+first\n",
    "\n",
    "'dxspeed_mean', 'dxspeed_std', 'data_x_return', 'data_y_unique_value_stdmean',#特征75 76\n",
    "\n",
    " 'time_delta_min', 'distance_deltas_max',  'median_speed', 'y_min', 'x_min', 'xs_delta_var', 'xs_delta_max',#开源部分特征\n",
    " 'dxacc_mean',    #715 feature\n",
    "          'dxacc_std', #gooooood\n",
    "'xt_angles_max', #0718\n",
    "     'xy_angles_kurtosis',#0719\n",
    "\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_train= train[feats2]\n",
    "final_test= test[feats2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final_train= train[feats2]\n",
    "# final_test= test_b[feats2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "final_train=final_train.fillna(-100)\n",
    "final_test=final_test.fillna(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('first_delt_t', 70), ('xt_angles_max', 60), ('median_speed', 55), ('get_target', 48), ('x_min', 37), ('data_y_unique_value_stdmean', 32), ('first_data_x', 29), ('time_delta_min', 27), ('dxspeed_std', 27), ('y_min', 24), ('first_data_y', 20), ('distance_deltas_max', 19), ('speed_xstd_laststd', 19), ('X_max', 18), ('xy_angles_kurtosis', 17), ('dxacc_mean', 16), ('first_speed_x', 15), ('dxspeed_mean', 11), ('xs_delta_max', 11), ('dxacc_std', 10), ('xs_delta_var', 6)]\n",
      "[('first_data_y', 43), ('first_delt_t', 43), ('xt_angles_max', 41), ('dxspeed_mean', 37), ('dxspeed_std', 37), ('first_data_x', 33), ('data_y_unique_value_stdmean', 30), ('time_delta_min', 30), ('dxacc_std', 29), ('get_target', 28), ('first_speed_x', 27), ('median_speed', 27), ('x_min', 22), ('dxacc_mean', 17), ('xs_delta_var', 13), ('speed_xstd_laststd', 12), ('xy_angles_kurtosis', 12), ('distance_deltas_max', 11), ('xs_delta_max', 11), ('X_max', 5), ('y_min', 3)]\n",
      "[('first_delt_t', 54), ('x_min', 45), ('xt_angles_max', 44), ('get_target', 40), ('dxspeed_std', 33), ('first_data_y', 32), ('first_speed_x', 28), ('first_data_x', 26), ('median_speed', 25), ('distance_deltas_max', 24), ('time_delta_min', 22), ('X_max', 21), ('speed_xstd_laststd', 21), ('data_y_unique_value_stdmean', 18), ('dxspeed_mean', 18), ('dxacc_std', 14), ('y_min', 12), ('dxacc_mean', 11), ('xs_delta_max', 11), ('xy_angles_kurtosis', 9), ('xs_delta_var', 7)]\n",
      "[('first_delt_t', 59), ('get_target', 46), ('first_data_x', 45), ('xt_angles_max', 40), ('median_speed', 35), ('first_data_y', 30), ('speed_xstd_laststd', 28), ('data_y_unique_value_stdmean', 27), ('dxspeed_mean', 22), ('time_delta_min', 20), ('x_min', 19), ('distance_deltas_max', 19), ('first_speed_x', 19), ('dxacc_mean', 19), ('xy_angles_kurtosis', 18), ('X_max', 17), ('dxspeed_std', 15), ('dxacc_std', 12), ('xs_delta_max', 8), ('xs_delta_var', 4), ('y_min', 3)]\n",
      "[('first_delt_t', 55), ('get_target', 48), ('xt_angles_max', 42), ('x_min', 40), ('median_speed', 36), ('first_data_y', 35), ('dxspeed_mean', 26), ('speed_xstd_laststd', 25), ('data_y_unique_value_stdmean', 24), ('dxspeed_std', 23), ('time_delta_min', 20), ('first_speed_x', 18), ('distance_deltas_max', 17), ('first_data_x', 17), ('dxacc_std', 16), ('xs_delta_max', 16), ('dxacc_mean', 13), ('xy_angles_kurtosis', 12), ('xs_delta_var', 9), ('X_max', 8), ('y_min', 4)]\n",
      "[('first_data_x', 64), ('first_delt_t', 53), ('first_data_y', 47), ('dxspeed_std', 37), ('get_target', 35), ('xt_angles_max', 35), ('data_y_unique_value_stdmean', 30), ('X_max', 29), ('median_speed', 29), ('distance_deltas_max', 28), ('time_delta_min', 24), ('dxacc_std', 22), ('first_speed_x', 19), ('speed_xstd_laststd', 19), ('xy_angles_kurtosis', 16), ('dxacc_mean', 14), ('dxspeed_mean', 13), ('x_min', 10), ('xs_delta_var', 7), ('y_min', 6), ('xs_delta_max', 5)]\n",
      "[('first_delt_t', 54), ('first_data_x', 49), ('get_target', 39), ('dxspeed_std', 34), ('data_y_unique_value_stdmean', 30), ('xt_angles_max', 30), ('time_delta_min', 29), ('first_data_y', 27), ('speed_xstd_laststd', 27), ('median_speed', 22), ('dxacc_std', 19), ('dxspeed_mean', 17), ('first_speed_x', 16), ('y_min', 11), ('distance_deltas_max', 11), ('X_max', 11), ('x_min', 10), ('xy_angles_kurtosis', 9), ('xs_delta_var', 7), ('xs_delta_max', 6), ('dxacc_mean', 3)]\n",
      "[('first_delt_t', 48), ('get_target', 46), ('first_data_x', 41), ('xt_angles_max', 38), ('first_data_y', 33), ('dxspeed_std', 33), ('data_y_unique_value_stdmean', 31), ('median_speed', 27), ('speed_xstd_laststd', 24), ('dxacc_std', 23), ('first_speed_x', 21), ('time_delta_min', 19), ('X_max', 18), ('dxspeed_mean', 18), ('xs_delta_max', 12), ('dxacc_mean', 12), ('distance_deltas_max', 11), ('xs_delta_var', 10), ('xy_angles_kurtosis', 10), ('x_min', 7), ('y_min', 6)]\n",
      "[('first_delt_t', 55), ('xt_angles_max', 49), ('get_target', 43), ('first_data_y', 41), ('first_data_x', 38), ('first_speed_x', 28), ('median_speed', 24), ('X_max', 18), ('distance_deltas_max', 17), ('dxspeed_mean', 17), ('time_delta_min', 16), ('y_min', 16), ('dxspeed_std', 16), ('xs_delta_var', 14), ('xy_angles_kurtosis', 14), ('dxacc_mean', 13), ('speed_xstd_laststd', 13), ('dxacc_std', 13), ('x_min', 9), ('data_y_unique_value_stdmean', 9), ('xs_delta_max', 9)]\n",
      "[('first_delt_t', 55), ('get_target', 52), ('xt_angles_max', 44), ('x_min', 37), ('data_y_unique_value_stdmean', 35), ('dxspeed_std', 31), ('first_data_y', 30), ('median_speed', 30), ('distance_deltas_max', 29), ('dxacc_std', 29), ('dxspeed_mean', 20), ('time_delta_min', 18), ('X_max', 18), ('y_min', 16), ('dxacc_mean', 14), ('first_speed_x', 13), ('speed_xstd_laststd', 12), ('xs_delta_var', 9), ('xy_angles_kurtosis', 8), ('first_data_x', 7), ('xs_delta_max', 6)]\n",
      "recall mean    : 0.96688  recall var     : 0.0001\n",
      "precision mean : 0.97459  precision var  : 0.00004\n",
      "score mean     :97.14228  score var      :0.12881\n",
      "[('first_delt_t', 107), ('xt_angles_max', 102), ('median_speed', 90), ('get_target', 81), ('dxspeed_std', 65), ('x_min', 64), ('first_data_x', 63), ('first_data_y', 60), ('distance_deltas_max', 53), ('dxspeed_mean', 52), ('data_y_unique_value_stdmean', 50), ('X_max', 50), ('xs_delta_var', 48), ('xs_delta_max', 43), ('time_delta_min', 42), ('speed_xstd_laststd', 38), ('y_min', 37), ('first_speed_x', 34), ('dxacc_std', 33), ('xy_angles_kurtosis', 32), ('dxacc_mean', 20)]\n",
      "negative number: 19798\n"
     ]
    }
   ],
   "source": [
    "params = {'threshold':0.65,'num_boost_rounds':150,\n",
    "'int_params':{ 'silent': 1,  'objective': 'binary:logistic' , \n",
    "   # 'gamma':0.2,\n",
    "       # 'min_child_weight':1,\n",
    "        # 'max_depth':5,\n",
    "        # 'lambda':10,\n",
    "        # 'subsample':0.7,\n",
    "        # 'colsample_bytree':0.7,\n",
    "        # 'colsample_bylevel':0.7,\n",
    "        # 'eta': 0.5,\n",
    "         'tree_method':'exact',\n",
    "        }}\n",
    "train_data1 ,train_label = final_train.drop(['id','label'], axis=1).astype(float),train.label\n",
    "\n",
    "\n",
    "xgbcv(params,train_data1,train_label)\n",
    "test_data1 = final_test.drop(['id','label'], axis=1).astype(float)\n",
    "\n",
    "final_test['label_prob'] = 0\n",
    "final_test['label_prob'] ,final_test[\"label\"]= xgb_pre(params,train_data1,train_label,test_data1)\n",
    "\n",
    "print(\"negative number:\",len(test)-final_test.label.sum())\n",
    "final_test[final_test.label>0]['id'].to_csv(\"submission/{0}.txt\".format(len(test)-final_test.label.sum()), header=None, index=False)\n",
    "# final_test=final_test.sort_values(by='label_prob')\n",
    "# final_test.iloc[0:20000].id.to_csv('submission/{0}_top2w.txt'.format(100000-final_test.label.sum()), header=None, index=False)\n",
    "# final_test.iloc[20000:100000].id.to_csv(\"submission/{0}_8W.txt\".format(100000-final_test.label.sum()), header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
